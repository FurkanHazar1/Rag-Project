# api_optimized.py - Performans optimize edilmi≈ü API
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, Any
import uvicorn
import logging
import time
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading
from functools import lru_cache
import gc

# Hata kontrol√º i√ßin
models_loaded = False
error_message = ""

try:
    from langchain_ollama import ChatOllama
    from langchain_community.embeddings import SentenceTransformerEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_core.prompts import ChatPromptTemplate
    from langchain.chains.llm import LLMChain
    from langchain.chains.combine_documents.stuff import StuffDocumentsChain
    from langchain.prompts import PromptTemplate
    from translation import translate_tr_to_en, translate_en_to_tr
    from nlp import QueryCleaner
    imports_ok = True
except Exception as e:
    imports_ok = False
    error_message = f"Import hatasƒ±: {str(e)}"
    print(f"‚ùå {error_message}")

# Logging ayarlarƒ± - performans i√ßin seviye d√º≈ü√ºr√ºld√º
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

app = FastAPI(title="Okul Mevzuat Chat API", version="1.0.0")

# CORS ayarlarƒ±
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response modelleri
class ChatRequest(BaseModel):
    question: str
    conversation_id: Optional[str] = None

class ChatResponse(BaseModel):
    answer: str
    conversation_id: str
    response_time: float
    success: bool
    error_message: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    models_loaded: bool
    version: str
    error_message: Optional[str] = None

# Global deƒüi≈ükenler
llm = None
retriever = None
combine_documents_chain = None
query_cleaner = None
executor = ThreadPoolExecutor(max_workers=4)  # Thread pool for blocking operations

# Cache'ler
@lru_cache(maxsize=100)
def cached_query_clean(query: str) -> str:
    """Query temizleme i≈ülemini cache'ler"""
    if query_cleaner:
        return query_cleaner.clean_query(query)
    return query

# Translation cache
translation_cache = {}
cache_lock = threading.Lock()

def cached_translate_tr_to_en(text: str) -> str:
    """√áeviri i≈ülemini cache'ler"""
    with cache_lock:
        if text in translation_cache:
            return translation_cache[text]
        
        result = translate_tr_to_en(text)
        if len(translation_cache) > 2000:
            with cache_lock:
                for _ in range(500):
                    if translation_cache:
                        translation_cache.popitem()
                
        translation_cache[text] = result
        return result

def cached_translate_en_to_tr(text: str) -> str:
    """√áeviri i≈ülemini cache'ler"""
    with cache_lock:
        cache_key = f"en_to_tr_{text}"
        if cache_key in translation_cache:
            return translation_cache[cache_key]
        
        result = translate_en_to_tr(text)
        if len(translation_cache) > 2000:
            for _ in range(500):
                translation_cache.popitem()
        
        translation_cache[cache_key] = result
        return result

# Fonksiyonlar (sadece import ba≈üarƒ±lƒ±ysa)
if imports_ok:
    def load_llm():
        # Ollama i√ßin optimizasyon ayarlarƒ±
        return ChatOllama(
            model="llama3.2", 
            temperature=0,
        )

    @lru_cache(maxsize=1)
    def get_faiss(model_name: str, index_path: str):
        """FAISS index'i cache'ler - bir kez y√ºkler"""
        embedding = SentenceTransformerEmbeddings(
            model_name=model_name,
            cache_folder="./sentence_transformer_cache"  # Cache klas√∂r√º belirt
        )
        vector = FAISS.load_local(
            index_path, embedding, allow_dangerous_deserialization=True
        )
        return vector

    @lru_cache(maxsize=1)
    def get_prompt_template():
        template = """
        1. Use the following pieces of context to answer the question at the end.
        2. If you don't know the answer, just say that "I am sorry, I don't know".
        3. Keep the answer crisp and concise.
        
        Context: {context}
        Question: {question}

        Helpful Answer:
        """
        return ChatPromptTemplate.from_template(template)

    class OptimizedTranslatedStuffDocumentsChain(StuffDocumentsChain):
        def _get_inputs(self, docs, **kwargs):
            # Paralel √ßeviri i≈ülemi
            translated_docs = []
            
            # √áeviri i≈ülemlerini paralel yap
            with ThreadPoolExecutor(max_workers=3) as trans_executor:
                translation_futures = []
                for doc in docs:
                    future = trans_executor.submit(cached_translate_tr_to_en, doc.page_content)
                    translation_futures.append((doc, future))
                
                for doc, future in translation_futures:
                    translated_content = future.result()
                    translated_doc = type(doc)(
                        page_content=translated_content,
                        metadata=doc.metadata
                    )
                    translated_docs.append(translated_doc)

            if 'question' in kwargs:
                kwargs['question'] = cached_translate_tr_to_en(kwargs['question'])

            return super()._get_inputs(translated_docs, **kwargs)

    async def answer_turkish_question_async(turkish_question: str) -> str:
        """Asenkron soru cevaplama"""
        loop = asyncio.get_event_loop()
        
        # CPU-intensive i≈ülemleri thread pool'da √ßalƒ±≈ütƒ±r
        def blocking_operation():
            # Query temizleme
            cleaned_query = cached_query_clean(turkish_question)
            
            # Dok√ºman arama - en alakalƒ± 3 dok√ºman al (5 yerine)
            retrieved_docs = retriever.get_relevant_documents(cleaned_query)[:5]
            
            # Chain √ßalƒ±≈ütƒ±rma
            result = combine_documents_chain.run(
                input_documents=retrieved_docs,
                question=turkish_question
            )
            
            # √áeviri
            response_clear = cached_translate_en_to_tr(result)
            return response_clear
        
        # Blocking i≈ülemi thread pool'da √ßalƒ±≈ütƒ±r
        result = await loop.run_in_executor(executor, blocking_operation)
        return result

# Startup i≈ülemi - optimize edilmi≈ü
@app.on_event("startup")
async def startup_event():
    global llm, retriever, combine_documents_chain, query_cleaner, models_loaded, error_message
    
    if not imports_ok:
        logger.error("‚ùå Import'lar ba≈üarƒ±sƒ±z, modeller y√ºklenmeyecek")
        return
    
    try:
        print("üîÑ Dosya kontrol√º yapƒ±lƒ±yor...")
        
        # Dosya kontrolleri
        model_name = 'intfloat/multilingual-e5-base'
        faiss_index_path = 'faiss_index'
        
        if not os.path.exists(faiss_index_path):
            raise Exception(f"FAISS index klas√∂r√º bulunamadƒ±: {faiss_index_path}")
        
        print("üì¶ Modeller y√ºkleniyor...")
        
        # Paralel model y√ºkleme
        async def load_models_parallel():
            loop = asyncio.get_event_loop()
            
            # LLM y√ºkleme
            llm_future = loop.run_in_executor(executor, load_llm)
            
            # FAISS y√ºkleme
            def load_faiss():
                vector = get_faiss(model_name, faiss_index_path)
                return vector.as_retriever(
                    search_type="similarity", 
                    search_kwargs={"k": 5}  # 5'ten 3'e d√º≈ü√ºr√ºld√º
                )
            
            faiss_future = loop.run_in_executor(executor, load_faiss)
            
            # QueryCleaner y√ºkleme
            def load_cleaner():
                return QueryCleaner()
            
            cleaner_future = loop.run_in_executor(executor, load_cleaner)
            
            # T√ºm√ºn√º bekle
            llm_result, retriever_result, cleaner_result = await asyncio.gather(
                llm_future, faiss_future, cleaner_future
            )
            
            return llm_result, retriever_result, cleaner_result
        
        llm, retriever, query_cleaner = await load_models_parallel()
        
        # Zincirler - optimize edilmi≈ü
        prompts = get_prompt_template()
        llm_chain = LLMChain(llm=llm, prompt=prompts, verbose=False)
        
        document_prompt = PromptTemplate(
            input_variables=["page_content", "source"],
            template="Context:\ncontent:{page_content}",  # source kaldƒ±rƒ±ldƒ±
        )
        
        combine_documents_chain = OptimizedTranslatedStuffDocumentsChain(
            llm_chain=llm_chain,
            document_variable_name="context",
            document_prompt=document_prompt,
        )
        
        models_loaded = True
        print("‚úÖ T√ºm modeller ba≈üarƒ±yla y√ºklendi!")
        
        # ƒ∞lk warm-up sorgusu
        try:
            await answer_turkish_question_async("test")
            print("‚úÖ Warm-up tamamlandƒ±")
        except:
            pass
        
    except Exception as e:
        error_message = str(e)
        logger.error(f"‚ùå Model y√ºkleme hatasƒ±: {error_message}")
        models_loaded = False

# Memory cleanup task
async def cleanup_memory():
    """Periyodik olarak memory temizliƒüi yapar"""
    while True:
        await asyncio.sleep(300)  # 5 dakikada bir
        gc.collect()
        # Cache boyutunu kontrol et
        if len(translation_cache) > 300:
            with cache_lock:
                for _ in range(100):
                    if translation_cache:
                        translation_cache.popitem()

@app.on_event("startup")
async def start_cleanup_task():
    asyncio.create_task(cleanup_memory())

# API Endpoints
@app.get("/")
async def root():
    return {"message": "Okul Mevzuat Chat API", "docs_url": "/docs", "models_loaded": models_loaded}

@app.get("/health", response_model=HealthResponse)
async def health_check():
    return HealthResponse(
        status="healthy" if models_loaded else "models_not_loaded",
        models_loaded=models_loaded,
        version="1.0.0",
        error_message=error_message if error_message else None
    )

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="Soru bo≈ü olamaz")
    
    if not models_loaded:
        return ChatResponse(
            answer="√úzg√ºn√ºm, AI modelleri hen√ºz y√ºklenmedi veya bir hata olu≈ütu. L√ºtfen sistem y√∂neticisine ba≈üvurun.",
            conversation_id=request.conversation_id or "error",
            response_time=0,
            success=False,
            error_message=error_message
        )
    
    try:
        start_time = time.time()
        
        # Asenkron soru cevaplama
        answer = await answer_turkish_question_async(request.question)
        
        response_time = time.time() - start_time
        
        return ChatResponse(
            answer=answer,
            conversation_id=request.conversation_id or "default",
            response_time=round(response_time, 2),
            success=True
        )
        
    except Exception as e:
        logger.error(f"Chat hatasƒ±: {str(e)}")
        return ChatResponse(
            answer="√úzg√ºn√ºm, bir hata olu≈ütu. L√ºtfen tekrar deneyin.",
            conversation_id=request.conversation_id or "default",
            response_time=0,
            success=False,
            error_message=str(e)
        )

# Batch processing endpoint (opsiyonel)
@app.post("/chat/batch")
async def chat_batch_endpoint(requests: list[ChatRequest]):
    """Birden fazla soruyu paralel i≈üler"""
    if not models_loaded:
        return {"error": "Models not loaded"}
    
    async def process_single_request(req):
        try:
            start_time = time.time()
            answer = await answer_turkish_question_async(req.question)
            response_time = time.time() - start_time
            
            return ChatResponse(
                answer=answer,
                conversation_id=req.conversation_id or "default",
                response_time=round(response_time, 2),
                success=True
            )
        except Exception as e:
            return ChatResponse(
                answer="Hata olu≈ütu.",
                conversation_id=req.conversation_id or "default",
                response_time=0,
                success=False,
                error_message=str(e)
            )
    
    # Paralel i≈üleme
    results = await asyncio.gather(*[process_single_request(req) for req in requests[:5]])  # Max 5 soru
    return {"results": results}

if __name__ == "__main__":
    print("üöÄ Optimize edilmi≈ü API ba≈ülatƒ±lƒ±yor...")
    uvicorn.run(
        app, 
        host="127.0.0.1", 
        port=8000, 
        reload=False,
        workers=1,  # Tek worker, async kullanƒ±yoruz
        loop="asyncio",
        access_log=False  # Access log'u kapat
    )